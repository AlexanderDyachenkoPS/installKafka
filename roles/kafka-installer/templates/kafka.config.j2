broker.id={{ hostvars[inventory_hostname]['myid'] }}
# change this to the hostname of each broker
#advertised.listeners=PLAINTEXT://{{ inventory_hostname }}:9092
# The ability to delete topics
delete.topic.enable=true
# Where logs are stored
log.dirs=/data/kafka
# default number of partitions
num.partitions=8
# default replica count based on the number of brokers
default.replication.factor=3
# to protect yourself against broker failure
min.insync.replicas=2
# logs will be deleted after how many hours
log.retention.hours=168
# size of the log files
log.segment.bytes=1073741824
# check to see if any data needs to be deleted
log.retention.check.interval.ms=300000
# location of all zookeeper instances and kafka directory
zookeeper.connect=192.168.0.240:2181,192.168.0.137:2181,192.168.0.156:2181/kafka
# timeout for connecting with zookeeper
zookeeper.connection.timeout.ms=6000
# automatically create topics
auto.create.topics.enable=true


# AUTH ACL
#sasl.enabled.mechanisms=PLAIN, SASL_PLAINTEXT

# Specify one of of the SASL mechanisms
#sasl.mechanism.inter.broker.protocol=SASL_PLAINTEXT
#security.inter.broker.protocol=SASL_PLAINTEXT


#listeners=SASL_PLAINTEXT://{{ inventory_hostname }}:9092
#advertised.listeners=SASL_PLAINTEXT://{{ inventory_hostname }}:9092

security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN
listeners=SASL_PLAINTEXT://{{ inventory_hostname }}:9092

confluent.metrics.reporter.sasl.mechanism=PLAIN
confluent.metrics.reporter.security.protocol=SASL_PLAINTEXT

#authorizer.class.name=io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
authorizer.class.name=kafka.security.authorizer.AclAuthorizer
#allow.everyone.if.no.acl.found=true
super.users=User:admin
#zookeeper.set.acl=true
#authorizer.class.name=org.apache.kafka.metadata.authorizer.StandardAuthorizer
